# ğŸ§ª Experiment 1: Transformer Encoder â€“ Autoencoding (Masked Language Model)

## ğŸ“Œ Objective
To understand and implement a Transformer Encoder and study:
- Self-Attention mechanism
- Autoencoding using Masked Language Modeling (MLM)
- Text reconstruction using masked words
- Visualization of attention weights

---

## ğŸ“– Problem Statement
1. Given an input sentence with a masked word (`[MASK]`), reconstruct the missing word.
2. Use a Transformer Encoder to learn contextual relationships between words.
3. Visualize self-attention weights to understand how the model focuses on different words.

Example:  
Input  : Transformers use [MASK] attention  
Output : Transformers use self attention  

---

## ğŸ“‚ Dataset
The experiment uses a dataset of 10 masked sentences.

ID | Masked Input Sentence | Expected Output
---|----------------------|----------------
1 | Transformers use [MASK] attention | self
2 | Mars is called the [MASK] planet | red
3 | Online learning improves [MASK] access | educational
4 | Exercise improves [MASK] health | mental
5 | Cricket is a [MASK] sport | popular
6 | Python is a [MASK] language | programming
7 | Neural networks have [MASK] layers | hidden
8 | Trees reduce [MASK] pollution | air
9 | Robots perform [MASK] tasks | repetitive
10 | Solar power is a [MASK] source | renewable

---

## ğŸ—ï¸ System Architecture

Input Sentence  
â†“  
Tokenization  
â†“  
Word Embedding  
â†“  
Self-Attention (Transformer Encoder)  
â†“  
Feed Forward Layer  
â†“  
Softmax  
â†“  
Masked Word Prediction  

---

## ğŸ” Autoencoding with Masked Language Model
Masked Language Modeling replaces one word in the sentence with `[MASK]`.  
The Transformer Encoder predicts the masked word using surrounding context, enabling autoencoding without recurrence.

---

## ğŸ”¬ Self-Attention Mechanism
Self-attention allows each word to attend to every other word in the sentence, capturing global dependencies efficiently.

---

## ğŸ“ Project Structure

transformer-encoder-autoencoding/  
â”œâ”€â”€ dataset.py  
â”œâ”€â”€ positional_encoding.py  
â”œâ”€â”€ attention.py  
â”œâ”€â”€ encoder.py  
â”œâ”€â”€ train_mlm.py  
â”œâ”€â”€ visualize_attention.ipynb  
â””â”€â”€ results/  
â€ƒâ€ƒâ””â”€â”€ attention_heatmap.png  

---

## âš™ï¸ Installation & Requirements

Requirements:
- Python 3.8+
- PyTorch
- NumPy
- Matplotlib
- Jupyter Notebook

Install dependencies:
pip install torch numpy matplotlib notebook

---

## â–¶ï¸ How to Run

Step 1: Train the model  
python train_mlm.py  

Step 2: Visualize attention  
jupyter notebook  
Open visualize_attention.ipynb  

---

## ğŸ“Š Attention Visualization

Below is the self-attention heatmap generated by the Transformer Encoder:

![Self-Attention Heatmap](results/attention_heatmap.png)

The heatmap shows how each word attends to others in the sentence.  
Brighter colors indicate stronger attention, explaining how the masked token uses surrounding context.

---

## ğŸ“ˆ Results

Sample Input:  
Transformers use [MASK] attention  

Output:  
Transformers use self attention  

All 10 masked sentences were reconstructed correctly.

---

## ğŸ“š Learning Outcomes
- Understanding Transformer Encoder architecture
- Knowledge of self-attention mechanism
- Practical experience with Masked Language Modeling
- Ability to visualize attention weights

---

## ğŸ†š Comparison with Traditional Models

Feature | RNN / CNN | Transformer
-------|-----------|------------
Parallel Processing | No | Yes
Long Dependency Handling | Weak | Strong
Attention Mechanism | No | Yes

---

## âœ… Conclusion
The Transformer Encoder effectively reconstructs masked text using self-attention and demonstrates superior contextual understanding compared to traditional models.

---

## ğŸ‘¨â€ğŸ“ Author
Name: Your Name  
Course: BE Computer Technology  
Institution: MIT Chennai  

---

## ğŸ“Œ Status
Experiment completed successfully.
