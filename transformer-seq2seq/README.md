# ğŸ§ª Experiment 2: Transformer Decoder â€“ Autoregression / Seq2Seq

## ğŸ“Œ Objective
To understand and implement a Transformer Decoder and Seq2Seq architecture with:
- Autoregression
- Causal masking
- Encoderâ€“Decoder attention
- Token-by-token text generation

---

## ğŸ“– Problem Statement
Build a Transformer-based Seq2Seq system that can:
- Generate paraphrases
- Answer questions
- Perform text generation

The model must generate output **autoregressively**, one token at a time, using causal masking.

---

## ğŸ“‚ Dataset
The experiment uses a small dataset of 10 inputâ€“output sentence pairs.

| Task Type | Input | Output |
|---------|------|--------|
| Paraphrase | AI improves healthcare | AI enhances medical diagnosis and treatment |
| Paraphrase | Transformers process data in parallel | Transformers handle sequences simultaneously |
| Q&A | What is self-attention? | Self-attention relates each word to every other word |
| Q&A | Why is positional encoding required? | Positional encoding provides word order information |
| Text Gen | In the future, AI will | In the future, AI will automate decision systems |
| Text Gen | Deep learning models can | Deep learning models can learn abstract features |
| Seq2Seq | Machine learning helps | Machine learning helps in data-driven decisions |
| Paraphrase | Attention improves NLP accuracy | Attention mechanisms increase NLP performance |
| Q&A | What is autoregression? | Autoregression generates output tokens sequentially |
| Text Gen | Transformers are useful because | Transformers are useful because they capture global context |

---

## ğŸ—ï¸ System Architecture

Input Sentence  
â†“  
Encoder (Embedding + Encoding)  
â†“  
Encoder Output (Memory)  
â†“  
Decoder  
â€¢ Masked Self-Attention  
â€¢ Encoderâ€“Decoder Cross-Attention  
â€¢ Feed Forward Network  
â†“  
Softmax  
â†“  
Generated Tokens (Autoregressive)

---

## ğŸ”¬ Key Concepts

### ğŸ”¹ Autoregression
The decoder generates output **one token at a time**, where each token depends on previously generated tokens.

### ğŸ”¹ Causal Masking
Causal masking prevents the decoder from seeing future tokens during generation, ensuring left-to-right prediction.

### ğŸ”¹ Encoderâ€“Decoder Attention
The decoder attends to encoder outputs, allowing the generated sequence to depend on the input sentence.

---

## ğŸ“ Project Structure

transformer-seq2seq/  
â”œâ”€â”€ dataset.py  
â”œâ”€â”€ attention_masks.py  
â”œâ”€â”€ encoder.py  
â”œâ”€â”€ decoder.py  
â”œâ”€â”€ transformer.py  
â”œâ”€â”€ train.py  
â”œâ”€â”€ inference.py  
â”œâ”€â”€ vocab.pkl  
â”œâ”€â”€ model.pth  
â””â”€â”€ results/  
â€ƒâ€ƒâ””â”€â”€ experiment2_output.png  

---

## âš™ï¸ Requirements

- Python 3.8+
- PyTorch
- NumPy

Install dependencies:
pip install torch numpy

---

## â–¶ï¸ How to Run

### Step 1: Train the Model
python train.py

### Step 2: Generate Output
python inference.py

---

## ğŸ§ª Experiment Output (Model Generated)

Below is the actual output generated by the Transformer Decoder during inference:

![Experiment 2 Output](results/experiment2_output.png)

The model generates text autoregressively using encoderâ€“decoder attention and causal masking.

---

## ğŸ“ˆ Observations
- The decoder generates output token by token.
- Causal masking prevents future token leakage.
- Encoderâ€“decoder attention ensures input context is preserved.
- Sampling techniques reduce repetition in generated text.

---

## ğŸ“š Learning Outcomes
- Understanding of Transformer Decoder architecture
- Knowledge of autoregressive text generation
- Practical implementation of causal masking
- Experience with Seq2Seq learning using Transformers

---

## ğŸ†š Comparison with Encoder-Only Model

| Feature | Encoder-Only | Encoderâ€“Decoder |
|------|--------------|----------------|
| Text Reconstruction | Yes | Yes |
| Autoregression | No | Yes |
| Seq2Seq Tasks | Limited | Full |
| Cross-Attention | No | Yes |

---

## âœ… Conclusion
The Transformer Decoder successfully generates coherent output sequences using autoregression and causal masking. This experiment demonstrates how modern language models perform Seq2Seq and text generation tasks effectively.

---

## ğŸ‘¨â€ğŸ“ Author
Name: Naveenkumar N  
Course: BE Computer Science and Engineering  
Institution: MIT Chennai  

---

## ğŸ“Œ Status
Experiment completed successfully.
