#  LLM Assignment â€“ Transformer Models (Encoder & Decoder)

##  Repository Overview
This repository contains implementations of Transformer-based Language Models as part of the LLM Assignment.  
The project is divided into two experiments, each focusing on a core Transformer architecture used in modern Large Language Models.

---

##  Experiments Included

###  Experiment 1: Transformer Encoder â€“ Autoencoding (Masked Language Model)
ğŸ“ Folder: transformer-encoder-autoencoding

Focus Areas:
- Transformer Encoder
- Self-Attention mechanism
- Masked Language Modeling (MLM)
- Autoencoding
- Attention visualization

Example:
Input  : Transformers use [MASK] attention  
Output : Transformers use self attention

---

###  Experiment 2: Transformer Decoder â€“ Autoregression / Seq2Seq
ğŸ“ Folder: transformer-seq2seq

Focus Areas:
- Transformer Decoder
- Autoregression
- Causal Masking
- Encoderâ€“Decoder (Seq2Seq)
- Token-by-token text generation

Example:
Input  : AI improves healthcare  
Output : AI enhances medical diagnosis and treatment

---

## ğŸ“‚ Repository Structure

LLM_Assignment/  
â”œâ”€â”€ transformer-encoder-autoencoding/  
â”‚   â”œâ”€â”€ dataset.py  
â”‚   â”œâ”€â”€ attention.py  
â”‚   â”œâ”€â”€ encoder.py  
â”‚   â”œâ”€â”€ train_mlm.py  
â”‚   â”œâ”€â”€ visualize_attention.ipynb  
â”‚   â”œâ”€â”€ README.md  
â”‚   â””â”€â”€ results/  
â”‚  
â”œâ”€â”€ transformer-seq2seq/  
â”‚   â”œâ”€â”€ dataset.py  
â”‚   â”œâ”€â”€ encoder.py  
â”‚   â”œâ”€â”€ decoder.py  
â”‚   â”œâ”€â”€ transformer.py  
â”‚   â”œâ”€â”€ attention_masks.py  
â”‚   â”œâ”€â”€ train.py  
â”‚   â”œâ”€â”€ inference.py  
â”‚   â”œâ”€â”€ README.md  
â”‚   â””â”€â”€ results/  
â”‚  
â””â”€â”€ README.md  

---

##  Technologies Used
- Python 3
- PyTorch
- NumPy
- Matplotlib
- Jupyter Notebook
- Git and GitHub

---

##  How to Run the Experiments

### Experiment 1
cd transformer-encoder-autoencoding  
python train_mlm.py  
jupyter notebook  

### Experiment 2
cd transformer-seq2seq  
python train.py  
python inference.py  

---

##  Learning Outcomes
- Understanding Transformer Encoder and Decoder architectures
- Difference between self-attention and cross-attention
- Masked Language Modeling
- Autoregressive text generation
- Causal masking in Transformers
- Seq2Seq learning using Encoderâ€“Decoder models

---

##  Conceptual Comparison

Feature | Encoder (Exp 1) | Encoderâ€“Decoder (Exp 2)
--------|----------------|------------------------
Masked Prediction | Yes | No
Autoregression | No | Yes
Seq2Seq Tasks | No | Yes
Cross-Attention | No | Yes
Text Generation | Limited | Full

---

##  Conclusion
This repository demonstrates the fundamental working principles of modern Large Language Models by implementing both encoder-only and encoderâ€“decoder Transformer architectures.  
The experiments provide practical insight into how models such as BERT and GPT operate internally.

---

##  Author
Name: Naveenkumar N 
Course: BE Computer Science and Engineering  
Institution: MIT Chennai  

---

##  Status
âœ” Experiment 1 Completed  
âœ” Experiment 2 Completed  
âœ” Code Verified  
âœ” Outputs Generated  
âœ” GitHub Submission Ready  
